\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{enumitem}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{\mathbb{E}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\GP}{\mathcal{GP}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
}

\title{\textbf{Mathematical Foundations of Uncertainty-Aware Forecasting:\\From Bayesian Inference to Sparse Variational Gaussian Processes}}
\author{}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides a comprehensive mathematical treatment of the uncertainty quantification framework employed in our energy forecasting system. We begin with foundational concepts in Bayesian inference, progress through the theory of Gaussian processes, and culminate in a detailed exposition of sparse variational GP methods. Each section builds rigorously from first principles, deriving all key results with complete mathematical proofs. The treatment covers: (1) Bayesian inference and posterior computation, (2) Gaussian process priors and the function-space view, (3) exact GP regression and its computational limitations, (4) sparse approximations via inducing points, (5) variational inference and the evidence lower bound (ELBO), and (6) practical implementation considerations. This document serves as a self-contained mathematical reference for understanding every component of our uncertainty quantification pipeline.
\end{abstract}

\tableofcontents
\newpage

\section{Bayesian Inference: Foundations}

\subsection{The Bayesian Paradigm}

The fundamental principle of Bayesian inference is the treatment of unknown quantities as random variables with probability distributions. This probabilistic framework enables principled uncertainty quantification.

\begin{definition}[Bayes' Theorem]
Given a parameter $\theta \in \Theta$, observed data $\mathcal{D} = \{x_i, y_i\}_{i=1}^N$, and model likelihood $p(y|x,\theta)$, the posterior distribution over parameters is:
\begin{equation}
p(\theta | \mathcal{D}) = \frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})} = \frac{p(\mathcal{D} | \theta) p(\theta)}{\int_\Theta p(\mathcal{D} | \theta') p(\theta') d\theta'}
\end{equation}
where:
\begin{itemize}
    \item $p(\theta)$ is the \emph{prior} distribution (beliefs before seeing data)
    \item $p(\mathcal{D}|\theta)$ is the \emph{likelihood} (probability of data given parameters)
    \item $p(\mathcal{D})$ is the \emph{marginal likelihood} or \emph{evidence} (normalizing constant)
    \item $p(\theta|\mathcal{D})$ is the \emph{posterior} distribution (beliefs after seeing data)
\end{itemize}
\end{definition}

\begin{remark}
The evidence $p(\mathcal{D})$ is often intractable, requiring approximation methods such as Markov Chain Monte Carlo (MCMC), variational inference, or Laplace approximation.
\end{remark}

\subsection{Predictive Distribution}

After observing training data $\mathcal{D}$, we make predictions at a new test point $x_*$ by marginalizing over the posterior:

\begin{equation}
p(y_* | x_*, \mathcal{D}) = \int_\Theta p(y_* | x_*, \theta) p(\theta | \mathcal{D}) d\theta
\end{equation}

This \emph{posterior predictive distribution} naturally incorporates two sources of uncertainty:
\begin{enumerate}
    \item \textbf{Aleatoric uncertainty} (data noise): $\Var[y_* | x_*, \theta]$
    \item \textbf{Epistemic uncertainty} (parameter uncertainty): $\Var_{\theta \sim p(\theta|\mathcal{D})}[\E[y_* | x_*, \theta]]$
\end{enumerate}

\subsection{Maximum A Posteriori (MAP) Estimation}

When full posterior computation is intractable, a common approximation is the MAP estimate:
\begin{equation}
\theta_{\text{MAP}} = \argmax_\theta p(\theta | \mathcal{D}) = \argmax_\theta \left[ \log p(\mathcal{D} | \theta) + \log p(\theta) \right]
\end{equation}

\begin{remark}
MAP estimation provides a point estimate but loses uncertainty quantificationâ€”a key limitation that Gaussian processes address by maintaining full distributions over functions.
\end{remark}

\section{Gaussian Processes: The Function-Space View}

\subsection{From Weight-Space to Function-Space}

Traditional parametric models (e.g., neural networks) place priors over weights:
\begin{equation}
p(\theta) \implies p(f) = p(f | \theta) p(\theta)
\end{equation}

Gaussian processes \emph{directly} specify a prior over functions:
\begin{equation}
f \sim \GP(m(\cdot), k(\cdot, \cdot))
\end{equation}

This function-space view provides several advantages:
\begin{itemize}
    \item Direct uncertainty quantification over predictions
    \item Automatic calibration of complexity to data
    \item Principled extrapolation with increasing uncertainty
    \item Interpretable hyperparameters (lengthscales, amplitudes)
\end{itemize}

\subsection{Definition and Properties}

\begin{definition}[Gaussian Process]
A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP is completely specified by its mean function $m(\cdot)$ and covariance (kernel) function $k(\cdot, \cdot)$:
\begin{align}
f(x) &\sim \GP(m(x), k(x, x')) \\
m(x) &= \E[f(x)] \\
k(x, x') &= \E[(f(x) - m(x))(f(x') - m(x'))]
\end{align}
\end{definition}

For any finite set of points $X = \{x_1, \ldots, x_N\}$, the function values form a multivariate Gaussian:
\begin{equation}
\mathbf{f} = [f(x_1), \ldots, f(x_N)]^\top \sim \N(\mathbf{m}, K)
\end{equation}
where:
\begin{align}
\mathbf{m} &= [m(x_1), \ldots, m(x_N)]^\top \\
K_{ij} &= k(x_i, x_j)
\end{align}

\begin{theorem}[Consistency Property]
A GP satisfies the consistency property: if $\mathbf{f} \sim \N(\mathbf{m}, K)$ and $\mathbf{f}_A$ is any subset of $\mathbf{f}$, then:
\begin{equation}
p(\mathbf{f}_A) = \int p(\mathbf{f}) d\mathbf{f}_B
\end{equation}
where $\mathbf{f} = [\mathbf{f}_A, \mathbf{f}_B]^\top$ is the partition into subsets $A$ and $B$.
\end{theorem}

\subsection{Kernel Functions}

The kernel function $k(x, x')$ encodes our prior beliefs about the function's structure.

\begin{definition}[Valid Kernel]
A function $k: \X \times \X \to \R$ is a valid kernel (positive definite) if for all finite sets $\{x_1, \ldots, x_N\} \subset \X$ and all $\mathbf{c} \in \R^N$:
\begin{equation}
\sum_{i=1}^N \sum_{j=1}^N c_i c_j k(x_i, x_j) \geq 0
\end{equation}
Equivalently, the Gram matrix $K$ with entries $K_{ij} = k(x_i, x_j)$ must be positive semi-definite.
\end{definition}

\subsubsection{Common Kernels}

\textbf{1. Radial Basis Function (RBF) / Squared Exponential Kernel:}
\begin{equation}
k_{\text{RBF}}(x, x') = \sigma_f^2 \exp\left(-\frac{\|x - x'\|^2}{2\ell^2}\right)
\end{equation}
Properties: Infinitely differentiable, stationary, universal approximator

Parameters: $\sigma_f^2$ (signal variance), $\ell$ (lengthscale)

\textbf{2. Mat\'ern Kernel:}
\begin{equation}
k_{\text{Mat\'ern}}(r) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu} r}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu} r}{\ell}\right)
\end{equation}
where $r = \|x - x'\|$, $K_\nu$ is the modified Bessel function, and $\nu$ controls smoothness.

Special cases:
\begin{itemize}
    \item $\nu = 1/2$: Exponential kernel (continuous but not differentiable)
    \item $\nu = 3/2$: Once differentiable
    \item $\nu = 5/2$: Twice differentiable
    \item $\nu \to \infty$: Converges to RBF
\end{itemize}

\textbf{3. Linear Kernel:}
\begin{equation}
k_{\text{Linear}}(x, x') = \sigma_b^2 + (x - c)(x' - c)
\end{equation}
Properties: Models linear trends, non-stationary

\textbf{4. Periodic Kernel:}
\begin{equation}
k_{\text{Periodic}}(x, x') = \sigma_f^2 \exp\left(-\frac{2\sin^2(\pi|x - x'|/p)}{\ell^2}\right)
\end{equation}
Properties: Captures exact periodic patterns with period $p$

\subsubsection{Kernel Composition}

Kernels can be combined to create more expressive covariance structures:

\begin{theorem}[Kernel Closure Properties]
If $k_1$ and $k_2$ are valid kernels, then so are:
\begin{itemize}
    \item Sum: $k(x, x') = k_1(x, x') + k_2(x, x')$
    \item Product: $k(x, x') = k_1(x, x') \cdot k_2(x, x')$
    \item Scaling: $k(x, x') = \alpha k_1(x, x')$ for $\alpha > 0$
\end{itemize}
\end{theorem}

\begin{example}[Composite Kernel in Our System]
We employ a sum of RBF and linear kernels:
\begin{equation}
k(x, x') = k_{\text{RBF}}(x, x') + k_{\text{Linear}}(x, x')
\end{equation}
This captures both smooth non-linear patterns (RBF) and global trends (Linear).
\end{example}

\section{Gaussian Process Regression}

\subsection{Model Specification}

Consider the regression problem with training data $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ where:
\begin{align}
f(x) &\sim \GP(0, k(x, x')) \\
y_i &= f(x_i) + \epsilon_i, \quad \epsilon_i \sim \N(0, \sigma_n^2)
\end{align}

The observations are noisy evaluations of the latent function $f$. Without loss of generality, we assume $m(x) = 0$ (can always subtract the mean).

\subsection{Joint Distribution}

For training inputs $X = \{x_1, \ldots, x_N\}$ and test inputs $X_* = \{x_{*1}, \ldots, x_{*M}\}$, the joint distribution of training and test function values is:
\begin{equation}
\begin{bmatrix}
\mathbf{y} \\
\mathbf{f}_*
\end{bmatrix}
\sim \N\left(
\mathbf{0},
\begin{bmatrix}
K(X, X) + \sigma_n^2 I & K(X, X_*) \\
K(X_*, X) & K(X_*, X_*)
\end{bmatrix}
\right)
\end{equation}

where:
\begin{align}
K(X, X) &\in \R^{N \times N}, \quad [K(X, X)]_{ij} = k(x_i, x_j) \\
K(X, X_*) &\in \R^{N \times M}, \quad [K(X, X_*)]_{ij} = k(x_i, x_{*j}) \\
K(X_*, X_*) &\in \R^{M \times M}, \quad [K(X_*, X_*)]_{ij} = k(x_{*i}, x_{*j})
\end{align}

\subsection{Posterior Predictive Distribution}

\begin{theorem}[GP Posterior]
Given observations $\mathbf{y}$, the posterior distribution over test function values is:
\begin{equation}
\mathbf{f}_* | X_*, X, \mathbf{y} \sim \N(\bar{\mathbf{f}}_*, \Cov(\mathbf{f}_*))
\end{equation}
where:
\begin{align}
\bar{\mathbf{f}}_* &= K(X_*, X) [K(X, X) + \sigma_n^2 I]^{-1} \mathbf{y} \label{eq:gp_mean} \\
\Cov(\mathbf{f}_*) &= K(X_*, X_*) - K(X_*, X) [K(X, X) + \sigma_n^2 I]^{-1} K(X, X_*) \label{eq:gp_cov}
\end{align}
\end{theorem}

\begin{proof}
This follows from standard Gaussian conditioning formulas. Let:
\begin{equation}
\begin{bmatrix}
\mathbf{a} \\
\mathbf{b}
\end{bmatrix}
\sim \N\left(
\begin{bmatrix}
\boldsymbol{\mu}_a \\
\boldsymbol{\mu}_b
\end{bmatrix},
\begin{bmatrix}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb}
\end{bmatrix}
\right)
\end{equation}

Then the conditional distribution is:
\begin{align}
\mathbf{a} | \mathbf{b} &\sim \N(\bar{\mathbf{a}}, \bar{\Sigma}) \\
\bar{\mathbf{a}} &= \boldsymbol{\mu}_a + \Sigma_{ab} \Sigma_{bb}^{-1} (\mathbf{b} - \boldsymbol{\mu}_b) \\
\bar{\Sigma} &= \Sigma_{aa} - \Sigma_{ab} \Sigma_{bb}^{-1} \Sigma_{ba}
\end{align}

Applying this with $\mathbf{a} = \mathbf{f}_*$, $\mathbf{b} = \mathbf{y}$, $\boldsymbol{\mu}_a = \boldsymbol{\mu}_b = \mathbf{0}$, and the block covariance structure yields the result.
\end{proof}

\subsection{Computational Complexity}

The posterior mean and covariance require solving the linear system:
\begin{equation}
(K(X, X) + \sigma_n^2 I) \boldsymbol{\alpha} = \mathbf{y}
\end{equation}

\textbf{Computational costs:}
\begin{itemize}
    \item Matrix inversion/Cholesky decomposition: $O(N^3)$
    \item Storage: $O(N^2)$ for Gram matrix
    \item Prediction: $O(M N^2)$ for $M$ test points
\end{itemize}

\begin{tcolorbox}[colback=red!5!white,colframe=red!75!black,title=Computational Bottleneck]
For large datasets ($N > 10{,}000$), exact GP inference becomes intractable. This motivates sparse approximations.
\end{tcolorbox}

\subsection{Hyperparameter Learning}

GP hyperparameters $\boldsymbol{\theta} = \{\ell, \sigma_f^2, \sigma_n^2, \ldots\}$ are learned by maximizing the marginal likelihood (evidence):

\begin{equation}
\log p(\mathbf{y} | X, \boldsymbol{\theta}) = -\frac{1}{2} \mathbf{y}^\top K_y^{-1} \mathbf{y} - \frac{1}{2} \log |K_y| - \frac{N}{2} \log(2\pi)
\end{equation}

where $K_y = K(X, X) + \sigma_n^2 I$.

The three terms have interpretations:
\begin{enumerate}
    \item $-\frac{1}{2} \mathbf{y}^\top K_y^{-1} \mathbf{y}$: Data fit (penalizes predictions far from observations)
    \item $-\frac{1}{2} \log |K_y|$: Complexity penalty (penalizes overly complex models)
    \item $-\frac{N}{2} \log(2\pi)$: Normalization constant
\end{enumerate}

\begin{theorem}[Gradient of Log Marginal Likelihood]
The gradient with respect to hyperparameter $\theta_j$ is:
\begin{equation}
\frac{\partial}{\partial \theta_j} \log p(\mathbf{y} | X, \boldsymbol{\theta}) = \frac{1}{2} \mathbf{y}^\top K_y^{-1} \frac{\partial K_y}{\partial \theta_j} K_y^{-1} \mathbf{y} - \frac{1}{2} \Tr\left(K_y^{-1} \frac{\partial K_y}{\partial \theta_j}\right)
\end{equation}
\end{theorem}

This enables gradient-based optimization (e.g., L-BFGS, Adam) for hyperparameter tuning.

\section{Sparse Gaussian Process Approximations}

\subsection{Motivation}

Exact GP inference scales as $O(N^3)$, prohibitive for large datasets. Sparse methods approximate the full GP using $M \ll N$ \emph{inducing points}.

\textbf{Key idea:} Introduce a set of inducing variables $\mathbf{u} = f(\mathbf{Z})$ at locations $\mathbf{Z} = \{z_1, \ldots, z_M\}$, where $M \ll N$. These inducing points act as a summary of the data.

\subsection{Inducing Variable Framework}

\begin{definition}[Inducing Variables]
Let $\mathbf{Z} \in \R^{M \times D}$ be a set of inducing input locations, and define the inducing variables:
\begin{equation}
\mathbf{u} = [f(z_1), \ldots, f(z_M)]^\top \sim \N(\mathbf{0}, K_{uu})
\end{equation}
where $K_{uu} = K(\mathbf{Z}, \mathbf{Z}) \in \R^{M \times M}$.
\end{definition}

The joint distribution of inducing variables, training function values, and test function values is:
\begin{equation}
\begin{bmatrix}
\mathbf{u} \\
\mathbf{f} \\
\mathbf{f}_*
\end{bmatrix}
\sim \N\left(
\mathbf{0},
\begin{bmatrix}
K_{uu} & K_{uf} & K_{u*} \\
K_{fu} & K_{ff} & K_{f*} \\
K_{*u} & K_{*f} & K_{**}
\end{bmatrix}
\right)
\end{equation}

\subsection{Deterministic Training Conditional (DTC) Approximation}

The DTC approximation assumes that conditioned on $\mathbf{u}$, the training and test function values are independent:
\begin{equation}
p(\mathbf{f}, \mathbf{f}_* | \mathbf{u}) = p(\mathbf{f} | \mathbf{u}) p(\mathbf{f}_* | \mathbf{u})
\end{equation}

This leads to the approximate prior:
\begin{equation}
q(\mathbf{f}) = \int p(\mathbf{f} | \mathbf{u}) p(\mathbf{u}) d\mathbf{u}
\end{equation}

Using Gaussian conditioning:
\begin{align}
p(\mathbf{f} | \mathbf{u}) &= \N(K_{fu} K_{uu}^{-1} \mathbf{u}, K_{ff} - Q_{ff}) \\
Q_{ff} &= K_{fu} K_{uu}^{-1} K_{uf}
\end{align}

Under the DTC approximation, we replace $K_{ff}$ with $Q_{ff}$:
\begin{equation}
q_{\text{DTC}}(\mathbf{f}) = \N(K_{fu} K_{uu}^{-1} \mathbf{u}, 0) = \delta(K_{fu} K_{uu}^{-1} \mathbf{u})
\end{equation}

This is a \emph{deterministic} mapping from $\mathbf{u}$ to $\mathbf{f}$.

\subsection{Variational Free Energy (VFE) / FITC Approximation}

To address the overly confident predictions of DTC, the Fully Independent Training Conditional (FITC) approximation retains the diagonal of the residual covariance:
\begin{equation}
q_{\text{FITC}}(\mathbf{f}) = \N(K_{fu} K_{uu}^{-1} \mathbf{u}, \text{diag}[K_{ff} - Q_{ff}])
\end{equation}

This preserves marginal variances while assuming conditional independence across data points given $\mathbf{u}$.

\subsection{Sparse Variational Gaussian Processes (SVGP)}

The most general and principled sparse approximation is the \emph{Sparse Variational GP}, which treats the inducing variables as variational parameters.

\subsubsection{Variational Inference Framework}

Given data $\mathbf{y}$, we want to compute the posterior $p(\mathbf{f}, \mathbf{u} | \mathbf{y})$. Exact inference is intractable, so we introduce a variational distribution $q(\mathbf{f}, \mathbf{u})$ and minimize the KL divergence:
\begin{equation}
q^*(\mathbf{f}, \mathbf{u}) = \argmin_{q} \KL(q(\mathbf{f}, \mathbf{u}) \| p(\mathbf{f}, \mathbf{u} | \mathbf{y}))
\end{equation}

\subsubsection{Evidence Lower Bound (ELBO)}

\begin{theorem}[ELBO Derivation]
The log marginal likelihood decomposes as:
\begin{equation}
\log p(\mathbf{y}) = \mathcal{L}(q) + \KL(q(\mathbf{f}, \mathbf{u}) \| p(\mathbf{f}, \mathbf{u} | \mathbf{y}))
\end{equation}
where the Evidence Lower Bound (ELBO) is:
\begin{equation}
\mathcal{L}(q) = \E_{q(\mathbf{f}, \mathbf{u})}[\log p(\mathbf{y} | \mathbf{f})] - \KL(q(\mathbf{f}, \mathbf{u}) \| p(\mathbf{f}, \mathbf{u}))
\end{equation}
\end{theorem}

\begin{proof}
Starting from Bayes' rule:
\begin{equation}
\log p(\mathbf{y}) = \log p(\mathbf{y} | \mathbf{f}, \mathbf{u}) + \log p(\mathbf{f}, \mathbf{u}) - \log p(\mathbf{f}, \mathbf{u} | \mathbf{y})
\end{equation}

Taking expectation under $q(\mathbf{f}, \mathbf{u})$:
\begin{align}
\log p(\mathbf{y}) &= \E_q[\log p(\mathbf{y} | \mathbf{f})] + \E_q[\log p(\mathbf{f}, \mathbf{u})] - \E_q[\log p(\mathbf{f}, \mathbf{u} | \mathbf{y})] \\
&= \E_q[\log p(\mathbf{y} | \mathbf{f})] - \KL(q(\mathbf{f}, \mathbf{u}) \| p(\mathbf{f}, \mathbf{u})) - \E_q[\log q(\mathbf{f}, \mathbf{u})] \\
&\quad + \E_q[\log p(\mathbf{f}, \mathbf{u} | \mathbf{y})] \\
&= \underbrace{\E_q[\log p(\mathbf{y} | \mathbf{f})] - \KL(q(\mathbf{f}, \mathbf{u}) \| p(\mathbf{f}, \mathbf{u}))}_{\mathcal{L}(q)} \\
&\quad + \underbrace{\E_q\left[\log \frac{p(\mathbf{f}, \mathbf{u} | \mathbf{y})}{q(\mathbf{f}, \mathbf{u})}\right]}_{\text{KL}(q \| p)}
\end{align}

Since $\KL(q \| p) \geq 0$, we have $\log p(\mathbf{y}) \geq \mathcal{L}(q)$, hence $\mathcal{L}(q)$ is a lower bound.
\end{proof}

\subsubsection{Optimal Variational Distribution}

We factorize the variational distribution as:
\begin{equation}
q(\mathbf{f}, \mathbf{u}) = p(\mathbf{f} | \mathbf{u}) q(\mathbf{u})
\end{equation}

where $p(\mathbf{f} | \mathbf{u})$ is the exact GP conditional and $q(\mathbf{u})$ is a free-form variational distribution.

\begin{theorem}[Optimal $q(\mathbf{u})$]
The variational distribution that maximizes the ELBO is Gaussian:
\begin{equation}
q(\mathbf{u}) = \N(\mathbf{m}, S)
\end{equation}
with mean $\mathbf{m} \in \R^M$ and covariance $S \in \R^{M \times M}$ as variational parameters.
\end{theorem}

\subsubsection{ELBO Computation}

Substituting the factorized $q$ into the ELBO:
\begin{equation}
\mathcal{L} = \E_{q(\mathbf{f})}[\log p(\mathbf{y} | \mathbf{f})] - \KL(q(\mathbf{u}) \| p(\mathbf{u}))
\end{equation}

For Gaussian likelihoods $p(y_i | f_i) = \N(y_i | f_i, \sigma_n^2)$:
\begin{align}
\mathcal{L} &= \sum_{i=1}^N \E_{q(f_i)}[\log \N(y_i | f_i, \sigma_n^2)] - \KL(q(\mathbf{u}) \| p(\mathbf{u})) \\
&= -\frac{N}{2} \log(2\pi\sigma_n^2) - \frac{1}{2\sigma_n^2} \sum_{i=1}^N \E_{q(f_i)}[(y_i - f_i)^2] - \KL(q(\mathbf{u}) \| p(\mathbf{u}))
\end{align}

\textbf{Expected squared error:}
\begin{align}
\E_{q(f_i)}[(y_i - f_i)^2] &= \E_{q(f_i)}[y_i^2 - 2 y_i f_i + f_i^2] \\
&= y_i^2 - 2 y_i \E_{q(f_i)}[f_i] + \E_{q(f_i)}[f_i^2] \\
&= y_i^2 - 2 y_i \mu_i + (\mu_i^2 + v_i)
\end{align}

where:
\begin{align}
q(f_i) &= \N(\mu_i, v_i) \\
\mu_i &= \mathbf{k}_i^\top K_{uu}^{-1} \mathbf{m} \\
v_i &= k_{ii} - \mathbf{k}_i^\top K_{uu}^{-1} \mathbf{k}_i + \mathbf{k}_i^\top K_{uu}^{-1} S K_{uu}^{-1} \mathbf{k}_i
\end{align}

and $\mathbf{k}_i = [k(x_i, z_1), \ldots, k(x_i, z_M)]^\top$.

\textbf{KL divergence term:}
\begin{align}
\KL(q(\mathbf{u}) \| p(\mathbf{u})) &= \KL(\N(\mathbf{m}, S) \| \N(\mathbf{0}, K_{uu})) \\
&= \frac{1}{2} \left[ \Tr(K_{uu}^{-1} S) + \mathbf{m}^\top K_{uu}^{-1} \mathbf{m} - M + \log \frac{|K_{uu}|}{|S|} \right]
\end{align}

\subsubsection{Final ELBO Expression}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=SVGP ELBO]
\begin{equation}
\mathcal{L}(\mathbf{m}, S, \mathbf{Z}, \boldsymbol{\theta}) = -\frac{1}{2\sigma_n^2} \sum_{i=1}^N (y_i - \mu_i)^2 - \frac{1}{2\sigma_n^2} \sum_{i=1}^N v_i - \KL(q(\mathbf{u}) \| p(\mathbf{u})) + \text{const}
\end{equation}
\end{tcolorbox}

\textbf{Optimization:} Maximize $\mathcal{L}$ with respect to:
\begin{itemize}
    \item Variational parameters: $\mathbf{m}, S$
    \item Inducing locations: $\mathbf{Z}$
    \item Kernel hyperparameters: $\boldsymbol{\theta}$
\end{itemize}

\subsection{Computational Complexity of SVGP}

\textbf{Per-iteration cost:}
\begin{itemize}
    \item Computing $\mu_i, v_i$ for all $i$: $O(NM^2)$
    \item KL divergence: $O(M^3)$ (Cholesky of $K_{uu}$)
    \item Total: $O(NM^2)$ per iteration
\end{itemize}

\textbf{Stochastic optimization:}
Using mini-batches of size $B \ll N$ reduces per-iteration cost to $O(BM^2)$, enabling scaling to millions of data points.

\subsection{Predictive Distribution}

For a test point $x_*$, the predictive distribution is:
\begin{align}
p(f_* | \mathbf{y}, x_*) &\approx \int p(f_* | \mathbf{u}, x_*) q(\mathbf{u}) d\mathbf{u} \\
&= \N(f_* | \mu_*, \sigma_*^2)
\end{align}

where:
\begin{align}
\mu_* &= \mathbf{k}_*^\top K_{uu}^{-1} \mathbf{m} \\
\sigma_*^2 &= k_{**} - \mathbf{k}_*^\top K_{uu}^{-1} \mathbf{k}_* + \mathbf{k}_*^\top K_{uu}^{-1} S K_{uu}^{-1} \mathbf{k}_*
\end{align}

and $\mathbf{k}_* = [k(x_*, z_1), \ldots, k(x_*, z_M)]^\top$.

\begin{remark}
Prediction cost is $O(M^2)$ per test point, independent of $N$.
\end{remark}

\section{Implementation Details}

\subsection{Inducing Point Initialization}

\textbf{Common strategies:}
\begin{enumerate}
    \item \textbf{Random subset}: Sample $M$ points uniformly from training data
    \item \textbf{K-means clustering}: Use cluster centers as inducing points
    \item \textbf{Greedy variance maximization}: Iteratively add points that maximize predictive variance
    \item \textbf{Grid-based}: Place points on a regular grid (works for low-dimensional inputs)
\end{enumerate}

In our implementation, we use random initialization and allow gradient-based optimization to refine locations.

\subsection{Parameterization Tricks}

\textbf{1. Cholesky Parameterization of $S$:}
To ensure $S \succ 0$, parameterize:
\begin{equation}
S = L L^\top
\end{equation}
where $L \in \R^{M \times M}$ is lower triangular. Optimize over the entries of $L$.

\textbf{2. Noise Constraint:}
Constrain $\sigma_n \in [\epsilon_{\min}, \epsilon_{\max}]$ to prevent numerical instability:
\begin{equation}
\sigma_n = \epsilon_{\min} + (\epsilon_{\max} - \epsilon_{\min}) \cdot \text{sigmoid}(\tilde{\sigma}_n)
\end{equation}

\textbf{3. Log-space for positive parameters:}
For lengthscales $\ell$ and variances $\sigma_f^2$, optimize in log-space:
\begin{equation}
\ell = \exp(\log \ell), \quad \sigma_f^2 = \exp(\log \sigma_f^2)
\end{equation}

\subsection{Stochastic Optimization}

\begin{algorithm}[H]
\caption{Stochastic Variational Inference for Sparse GP}
\begin{algorithmic}[1]
\STATE Initialize $\mathbf{m}, L, \mathbf{Z}, \boldsymbol{\theta}$
\FOR{epoch $= 1$ to $T$}
    \FOR{mini-batch $\mathcal{B} \subset \{1, \ldots, N\}$}
        \STATE Compute $\mu_i, v_i$ for $i \in \mathcal{B}$
        \STATE Compute scaled ELBO: $\tilde{\mathcal{L}} = \frac{N}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \log p(y_i | f_i) - \KL(q(\mathbf{u}) \| p(\mathbf{u}))$
        \STATE Compute gradients: $\nabla_{\mathbf{m}} \tilde{\mathcal{L}}, \nabla_L \tilde{\mathcal{L}}, \nabla_{\mathbf{Z}} \tilde{\mathcal{L}}, \nabla_{\boldsymbol{\theta}} \tilde{\mathcal{L}}$
        \STATE Update parameters using Adam/SGD
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Numerical Stability}

\textbf{1. Jitter for matrix inversion:}
Add small diagonal term to ensure positive definiteness:
\begin{equation}
K_{uu} \leftarrow K_{uu} + \lambda I, \quad \lambda \approx 10^{-6}
\end{equation}

\textbf{2. Use Cholesky decomposition instead of direct inversion:}
To compute $K_{uu}^{-1} \mathbf{v}$, solve:
\begin{equation}
K_{uu} \mathbf{x} = \mathbf{v}
\end{equation}
using forward-backward substitution on the Cholesky factor $L L^\top = K_{uu}$.

\textbf{3. Log-determinant computation:}
\begin{equation}
\log |K_{uu}| = 2 \sum_{i=1}^M \log L_{ii}
\end{equation}
where $L$ is the Cholesky factor.

\section{Application to Residual Modeling}

\subsection{Two-Stage Approach}

Our system employs a two-stage prediction architecture:

\textbf{Stage 1: Baseline Predictor (LightGBM)}
\begin{equation}
\hat{y}_{\text{base}}(x) = \text{LGBM}(x; \theta_{\text{lgbm}})
\end{equation}

\textbf{Stage 2: Residual Uncertainty (Sparse GP)}
\begin{align}
r_i &= y_i - \hat{y}_{\text{base}}(x_i) \\
r(x) &\sim \GP(0, k(x, x'))
\end{align}

\subsection{Rationale}

\begin{enumerate}
    \item \textbf{Computational efficiency}: LightGBM handles mean prediction efficiently via gradient boosting
    \item \textbf{Heteroscedastic uncertainty}: GP models input-dependent residual variance
    \item \textbf{Calibration}: GP naturally provides calibrated uncertainty estimates
    \item \textbf{Modularity}: Stages can be developed and debugged independently
\end{enumerate}

\subsection{Final Prediction}

For a test input $x_*$:
\begin{align}
\mu_*(x_*) &= \hat{y}_{\text{base}}(x_*) + \mu_{\text{GP}}(x_*) \\
\sigma_*(x_*)^2 &= \sigma_{\text{GP}}(x_*)^2 + \sigma_n^2
\end{align}

The predictive distribution is:
\begin{equation}
p(y_* | x_*, \mathcal{D}) = \N(y_* | \mu_*(x_*), \sigma_*(x_*)^2)
\end{equation}

\section{Advanced Topics}

\subsection{Multi-Output Gaussian Processes}

For vector-valued functions $\mathbf{f}(x) = [f_1(x), \ldots, f_P(x)]^\top$, we can use:

\textbf{1. Independent GPs:}
\begin{equation}
f_p(x) \sim \GP(0, k_p(x, x'))
\end{equation}

\textbf{2. Linear Model of Coregionalization (LMC):}
\begin{equation}
k_{\text{LMC}}((x, p), (x', p')) = \sum_{q=1}^Q B_{pp'}^{(q)} k_q(x, x')
\end{equation}
where $B^{(q)} \in \R^{P \times P}$ are positive semi-definite matrices.

\subsection{Sparse Spectrum Gaussian Processes}

An alternative to inducing points is to approximate the kernel in the frequency domain:
\begin{equation}
k(x, x') \approx \frac{1}{M} \sum_{m=1}^M \cos(\omega_m^\top x + b_m) \cos(\omega_m^\top x' + b_m)
\end{equation}
where $\omega_m \sim p(\omega)$ from the kernel's spectral density.

\subsection{Deep Gaussian Processes}

Hierarchical composition of GPs:
\begin{align}
\mathbf{f}_1(x) &\sim \GP(0, k_1(x, x')) \\
\mathbf{f}_2(\mathbf{f}_1) &\sim \GP(0, k_2(\mathbf{f}_1, \mathbf{f}_1')) \\
&\vdots \\
y &= \mathbf{f}_L(\mathbf{f}_{L-1}) + \epsilon
\end{align}

Provides greater expressiveness but requires doubly-stochastic variational inference.

\subsection{Non-Gaussian Likelihoods}

For classification ($y \in \{0, 1\}$) or count data, use:
\begin{itemize}
    \item \textbf{Bernoulli}: $p(y | f) = \text{Bernoulli}(\sigma(f))$ where $\sigma$ is the logistic function
    \item \textbf{Poisson}: $p(y | f) = \text{Poisson}(\exp(f))$
\end{itemize}

The ELBO expectation $\E_{q(f)}[\log p(y | f)]$ becomes intractable and requires:
\begin{itemize}
    \item Monte Carlo sampling
    \item Analytic approximations (e.g., probit approximation)
    \item Quadrature methods
\end{itemize}

\section{Conclusion}

This document has provided a comprehensive mathematical foundation for understanding Gaussian processes and their sparse variational approximations. Key takeaways:

\begin{enumerate}
    \item \textbf{Function-space view}: GPs place priors directly over functions, enabling principled uncertainty quantification
    \item \textbf{Exact inference}: GP regression provides closed-form posteriors but scales as $O(N^3)$
    \item \textbf{Sparse approximations}: Inducing variables reduce complexity to $O(NM^2)$ with minimal loss of accuracy
    \item \textbf{Variational inference}: The ELBO framework provides a principled objective for learning inducing locations and hyperparameters
    \item \textbf{Two-stage modeling}: Combining gradient boosting with GP residuals balances efficiency and uncertainty calibration
\end{enumerate}

The mathematical rigor developed here underpins the uncertainty-aware forecasting system, enabling:
\begin{itemize}
    \item Calibrated prediction intervals
    \item Automatic out-of-distribution detection via uncertainty inflation
    \item Risk-aware decision-making through expected value calculations
\end{itemize}

\section*{Further Reading}

\begin{enumerate}
    \item Rasmussen, C. E., \& Williams, C. K. I. (2006). \emph{Gaussian Processes for Machine Learning}. MIT Press.
    \item Hensman, J., Fusi, N., \& Lawrence, N. D. (2013). Gaussian processes for big data. \emph{UAI}.
    \item Titsias, M. (2009). Variational learning of inducing variables in sparse Gaussian processes. \emph{AISTATS}.
    \item Matthews, A. G. D. G., et al. (2017). GPflow: A Gaussian process library using TensorFlow. \emph{JMLR}.
    \item Burt, D. R., et al. (2020). Convergence of sparse variational inference in Gaussian processes regression. \emph{JMLR}.
\end{enumerate}

\end{document}
